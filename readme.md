<h1>Description for each assignment</h1>

<h2>A1</h2>
<p><ol>Image denoising with L2 norm regularization using closed form solution</ol></p>
<h2>A2</h2>
<p><ol>Image denoising with L2 norm regularization using gradient descent with Lipschitz constant as the step size</ol>
<ol>
gradient descent with line-search
</ol>
<ol>
gradient descent with Armijo line-search
</ol>
</p>
<h2>A3</h2>
<p><ol>
Image denoising with L1 norm regularization using gradient descent with Armijo line-search. Use the pseudo-Huber function to smooth the problem
</ol>
<ol>
Gradient descent with simple line-search
</ol>
<ol>
Accelerated gradient descent with Lipschitz constant
</ol>
<ol>
Accelerated gradient descent with Armijo line-search
</ol>
</p>
<h2>A4</h2>
<p>use Hing-loss and L2-regularized logistic regression as objective function to model classifier. And apply the classifier to three different datasets. The last two datasets are big. cannot upload to github</p>
<p>apply following methods to train: 
<ol>1) Stochastic sub-gradient</ol>
<ol>2) Stochastic gradient</ol>
<ol>3) Mini-batch (sub-)gradient</ol>
<ol>4) Stochastic average sub-gradient (SAG)</ol>
<ol>5) Stochastic average gradient (SAG)</ol>
<ol>6) Gradient descent with Armijo line-search</ol>
<ol>7) Acceleratd gradient with Armijo line-search </ol>
</p>
<h2>A5</h2>
<p>solve l1-regularized logistic regression problem using
<ol>
Proximal gradient descent

Accelerated proximal gradient descent
Proximal coordinate descent
Accelerated proximal coordinate descent</ol>
</p>
<p>label propagation using coordinate descent</p>
<p>local graph clustering using page ranking</p>
<p>Reccomender system using proximal gradient descent</p>
<p>Nonnegative Matrix Factorization: facial feature extraction</p>
<h2>A6</h2>
<p>Use ADMM to separate a background image form foreground interference</p>
<p>Implement AM-RR (alternating minimization for robust regresion) on the same dataset as Q1. Form a matrix X whose columns are the first 70 bird images. Form a vector y that is the 71st image. Then try to fit y = Xw using the Robust Linear Regression problem.</p>